{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import tqdm\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from models.data import MNISTDataSet\n",
    "from models.mnist import MNISTPredictor\n",
    "from models.utils import (\n",
    "    set_seed,\n",
    "    validate_model,\n",
    "    get_dataloaders,\n",
    "    loss,\n",
    "    ArgsDict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset.\n",
    "\n",
    "Note that we did not create dataloader for train, validation, test because splitting dataset requires hyperparamter `seed` and `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = MNISTDataSet(\"./data/mnist_train.csv\")\n",
    "test_dataset = MNISTDataSet(\"./data/mnist_test.csv\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "shape = (1, 28, 28)\n",
    "n_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Train loop.\n",
    "\n",
    "Each train loop will be executed with different hyperparameters such as learning rate, batch size and so on.\n",
    "\n",
    "We will pass `arg` which includes all hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Tensorboard is usually used to log the training process with training accuracy and valid accuracy.\n",
    "\n",
    "Besids accuracies, we also log many things as below.\n",
    "- model graph\n",
    "- historgrams of model parameters and their gradients over steps\n",
    "- images (e.g., hand-written digits in this example)\n",
    "- tables\n",
    "- profiling\n",
    "\n",
    "I will put TB tags for the parts required for the tensorboard logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = f\"{args.log_root_dir}/{current_time}\"\n",
    "    tb = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    train_dataset, valid_dataset = train_test_split(\n",
    "        train_valid_dataset, test_size=0.2, random_state=args.seed, shuffle=True\n",
    "    )\n",
    "    model = MNISTPredictor(shape, args.latent_dim, n_class=n_class)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.0001,\n",
    "    )\n",
    "    train_dataloader, valid_dataloader, test_dataloader = get_dataloaders(\n",
    "        train_dataset, valid_dataset, test_dataset, args.batch_size\n",
    "    )\n",
    "\n",
    "    for epoch in range(args.max_epoch):\n",
    "        model.train()\n",
    "        train_loss, acc_cnt, n = 0.0, 0, 0\n",
    "\n",
    "        for X, y_val in tqdm.tqdm(train_dataloader, ncols=50):\n",
    "            X = X.to(device)\n",
    "            y_val = y_val.to(torch.int64).to(device)\n",
    "            y_prob, y_pred = model(X)\n",
    "            y_true = (\n",
    "                torch.nn.functional.one_hot(y_val, num_classes=n_class)\n",
    "                .squeeze()\n",
    "                .float()\n",
    "                .to(device)\n",
    "            )\n",
    "\n",
    "            l = loss(y_prob, y_true).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += l.cpu().item()\n",
    "            n += X.shape[0]\n",
    "            acc_cnt += torch.sum(y_pred == y_val)\n",
    "\n",
    "        train_loss /= n\n",
    "        acc = acc_cnt / n\n",
    "\n",
    "        # Log loss and accuracy.\n",
    "        tb.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "        tb.add_scalar(\"train/acc\", acc, epoch)\n",
    "\n",
    "        valid_loss, valid_acc = validate_model(model, valid_dataloader, n_class)\n",
    "        tb.add_scalar(\"valid/loss\", valid_loss, epoch)\n",
    "        tb.add_scalar(\"valid/acc\", valid_acc, epoch)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # log how the parameters change over training steps\n",
    "            for name, weight in model.named_parameters():\n",
    "                tb.add_histogram(name, weight, epoch + 1)\n",
    "                tb.add_histogram(f\"{name}.grad\", weight.grad, epoch + 1)\n",
    "\n",
    "            # log y_val and y_pred with the actual figures\n",
    "\n",
    "    test_loss, test_acc = validate_model(model, test_dataloader, n_class)\n",
    "\n",
    "    # we can check performance across different hyperparameters\n",
    "    # NOTE: if run_name is not specified this way, then it will generate seperate dirs to store metrics for hparams.\n",
    "    tb.add_hparams(\n",
    "        {\"lr\": args.lr, \"bsize\": args.batch_size, \"latent_dim\": args.latent_dim},\n",
    "        {\"test/loss\": test_loss, \"test/acc\": test_acc},\n",
    "        run_name=os.path.dirname(os.path.realpath(__file__)) + os.sep + tb.log_dir,\n",
    "    )\n",
    "\n",
    "    # tensorboard draws the model's graph structure.\n",
    "    # NOTE: add_graph requires input shape to the model.\n",
    "    tb.add_graph(model, next(iter(test_dataloader))[0][[0]].to(device))\n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with combinations of hyperparameters\n",
    "\n",
    "We can run multiple experiments with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 187/187 [00:02<00:00, 73.78it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 535.72it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 134.88it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 547.79it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 133.23it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 328.49it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 132.97it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 462.42it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 128.64it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 414.15it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 125.16it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 492.68it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 135.37it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 537.09it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 133.27it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 467.93it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 130.99it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 487.67it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 131.38it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 481.75it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 132.33it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 543.79it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 133.93it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 523.79it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 132.66it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 485.65it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 131.49it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 378.24it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 126.89it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 538.18it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 132.92it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 534.29it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 134.82it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 540.53it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 134.36it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 492.84it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 148.66it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 519.17it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 131.72it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 542.13it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 134.50it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 524.07it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 134.11it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 532.29it/s]\n",
      "100%|██████████| 187/187 [00:01<00:00, 133.75it/s]\n",
      "100%|████████████| 46/46 [00:00<00:00, 475.99it/s]\n",
      " 71%|███████   | 133/187 [00:01<00:00, 132.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m run_idx, (batch_size, lr, latent_dim) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(product(\u001b[39m*\u001b[39mparam_values)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     args\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: batch_size, \u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: lr, \u001b[39m\"\u001b[39m\u001b[39mlatent_dim\u001b[39m\u001b[39m\"\u001b[39m: latent_dim})\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     train(args)\n",
      "\u001b[1;32m/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m l\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B220.149.232.227/home/cis_user1/doyun/pytorch-loggers/train_with_tensorboard.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m acc_cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(y_pred \u001b[39m==\u001b[39m y_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = dict(\n",
    "    batch_size=[128, 256],\n",
    "    lr=[1e-2, 1e-3],\n",
    "    latent_dim=[4, 8, 16],\n",
    "    # batch_size=[256],\n",
    "    # lr=[1e-3],\n",
    "    # latent_dim=[8],\n",
    ")\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "\n",
    "args = ArgsDict({\"seed\": 0, \"max_epoch\": 50, \"log_root_dir\": \"mnist-tb\"})\n",
    "\n",
    "for run_idx, (batch_size, lr, latent_dim) in enumerate(product(*param_values)):\n",
    "    args.update({\"batch_size\": batch_size, \"lr\": lr, \"latent_dim\": latent_dim})\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
